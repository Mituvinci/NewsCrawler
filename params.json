{
  "name": "Newscrawler",
  "tagline": "News crawler",
  "body": "# Bangladeshi Online Newspaper Crawler\r\n\r\n## Currently working on:\r\n\r\n* [~~en.prothom-alo.com~~](http://en.prothom-alo.com)\r\n* [~~thedailystar.net~~](http://www.thedailystar.net)\r\n* [dhakatribune.com](http://archive.dhakatribune.com/archive)\r\n\r\n# Requirements\r\n\r\n* Python 2k\r\n* Check `requirements.txt` file to find out about the dependencies\r\n\r\n# How to \r\n\r\n## 0. Before Beginning\r\n\r\n### 1. [Download and Install MongoDB](https://docs.mongodb.com/v3.2/installation/) \r\n### 2. [Download Stanford NER](http://nlp.stanford.edu/software/CRF-NER.shtml) and [configure it](https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a) \r\n### 3. [Download and configure Elasticsearch & Kibana](https://www.elastic.co/guide/index.html)\r\n\r\n## 1. Installing the dependencies\r\n\r\nClone the repository, then at the root of the directory of the repo, open a command window/terminal and run this following command. Make sure you have `pip`.\r\n\r\n```\r\npip install -r requirements.txt\r\n```\r\n\r\n## 2. Configuring API and StanfordNER Path\r\n\r\n### Indicoio API Configuration\r\n\r\nAt `credentials_and_configs/keys.py` file, change the API key. **[You can create an account here and get your own API Key](https://indico.io/)**.\r\n\r\nExample,\r\n```\r\nINDICOIO_API_KEY = '8ee6432e7dc137740c40c0af8d7XXXXXX' # Replace the value with your own API Key\r\n```\r\n\r\n### StanfordNER Path \r\n\r\nAt `credentials_and_configs/stanford_ner_path.py` file, change the paths according to the downloaded `NER` and `CLASSIFIER` paths.\r\n\r\nExample,\r\n```\r\nSTANFORD_NER_PATH = 'C:\\StanfordParser\\stanford-ner-2015-12-09\\stanford-ner.jar' #Insert your path here\r\nSTANFORD_CLASSIFIER_PATH = 'C:\\StanfordParser\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz' # Insert your path here\r\n```\r\n\r\n## 3. Running the spiders\r\n\r\nOpen a command `window / terminal` at the `root` of the folder. Run the following commands to start scraping.\r\n\r\n### 4. Crawling Instructions\r\n\r\n## Spider Names\r\n\r\n* The Daily Star -> `dailystar`\r\n* Prothom Alo -> `prothomalo`\r\n* Dhaka Tribune -> `dhakatribune`\r\n\r\n#### Crawl 'em all\r\n\r\n**For Daily Star**\r\n```\r\nscrapy crawl dailystar\r\n```\r\n\r\n**For Prothom Alo**\r\n```\r\nscrapy crawl prothomalo\r\n```\r\n\r\n**For Dhaka Tribune**\r\n```\r\nscrapy crawl dhakatribune\r\n```\r\n\r\n#### Crawling bounded by date time \r\n\r\nIf I want to scrape all of the news between `1st January 2016` and `1st February 2016` my command will look like this, \r\n\r\n**Daily Star**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n**Prothom Alo**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n#### Crawling Dhaka Tribune by page range\r\n\r\n**Dhaka Tribune**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10\r\n```\r\n\r\n#### Crawling with CSV/JSON output \r\n\r\nIf you want to collect all crawled data in a csv or a json file you can run this command.\r\n\r\n**Daily Star [`csv` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Daily Star [`json` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n**Dhaka Tribune [`json` output]**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.csv\r\n```\r\n\r\n**Dhaka Tribune [`csv` output]**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.json\r\n```\r\n\r\n**Prothom Alo [`csv` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Prothom Alo [`json` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n\r\n## 5. Data insertion into Elasticsearch and Kibana Visualization Instructions\r\n\r\n### [TODO]",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}