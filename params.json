{
  "name": "Newscrawler",
  "tagline": "News crawler",
  "body": "# Bangladeshi Online Newspaper Crawler\r\n\r\n## Done\r\n\r\n* [~~en.prothom-alo.com~~](http://en.prothom-alo.com)\r\n* [~~thedailystar.net~~](http://www.thedailystar.net)\r\n* [~~dhakatribune.com~~](http://archive.dhakatribune.com/archive)\r\n\r\n# Requirements\r\n\r\n* Python 2k\r\n* Check `requirements.txt` file to find out about the dependencies\r\n\r\n# How to \r\n\r\n## 0. Before Beginning\r\n\r\n### 1. [Download and Install MongoDB](https://docs.mongodb.com/v3.2/installation/) \r\n### 2. [Download Stanford NER](http://nlp.stanford.edu/software/CRF-NER.shtml) and [configure it](https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a) \r\n### 3. [Download and configure Elasticsearch & Kibana](https://www.elastic.co/guide/index.html)\r\n\r\n## 1. Installing the dependencies\r\n\r\nClone the repository, then at the root of the directory of the repo, open a command window/terminal and run this following command. Make sure you have `pip`.\r\n\r\n```\r\npip install -r requirements.txt\r\n```\r\n\r\n## 2. Configuring API and StanfordNER Path\r\n\r\n### Indicoio API Configuration\r\n\r\nAt `credentials_and_configs/keys.py` file, change the API key. **[You can create an account here and get your own API Key](https://indico.io/)**.\r\n\r\nExample,\r\n```\r\nINDICOIO_API_KEY = '8ee6432e7dc137740c40c0af8d7XXXXXX' # Replace the value with your own API Key\r\n```\r\n\r\n### StanfordNER Path \r\n\r\nAt `credentials_and_configs/stanford_ner_path.py` file, change the paths according to the downloaded `NER` and `CLASSIFIER` paths.\r\n\r\nExample,\r\n```\r\nSTANFORD_NER_PATH = 'C:\\StanfordParser\\stanford-ner-2015-12-09\\stanford-ner.jar' #Insert your path here\r\nSTANFORD_CLASSIFIER_PATH = 'C:\\StanfordParser\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz' # Insert your path here\r\n```\r\n\r\n## 3. Running the spiders\r\n\r\nOpen a command `window / terminal` at the `root` of the folder. Run the following commands to start scraping.\r\n\r\n### 4. Crawling Instructions\r\n\r\n## Spider Names\r\n\r\n* The Daily Star -> `dailystar`\r\n* Prothom Alo -> `prothomalo`\r\n* Dhaka Tribune -> `dhakatribune`\r\n\r\n#### Crawl 'em all\r\n\r\n**For Daily Star**\r\n```\r\nscrapy crawl dailystar\r\n```\r\n\r\n**For Prothom Alo**\r\n```\r\nscrapy crawl prothomalo\r\n```\r\n\r\n**For Dhaka Tribune**\r\n```\r\nscrapy crawl dhakatribune\r\n```\r\n\r\n#### Crawling bounded by date time \r\n\r\nIf I want to scrape all of the news between `1st January 2016` and `1st February 2016` my command will look like this, \r\n\r\n**Daily Star**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n**Prothom Alo**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n#### Crawling Dhaka Tribune by page range\r\n\r\n**Dhaka Tribune**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10\r\n```\r\n\r\n#### Crawling with CSV/JSON output \r\n\r\nIf you want to collect all crawled data in a csv or a json file you can run this command.\r\n\r\n**Daily Star [`csv` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Daily Star [`json` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n**Dhaka Tribune [`csv` output]**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.csv\r\n```\r\n\r\n**Dhaka Tribune [`json` output]**\r\n```\r\nscrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.json\r\n```\r\n\r\n**Prothom Alo [`csv` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Prothom Alo [`json` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n\r\n## 5. Data insertion into Elasticsearch and Kibana Visualization Instructions\r\n\r\n* Download and extract Kibana and Elasticsearch\r\n\r\n### Starting MongoDB Service\r\n\r\n* Open CMD/Terminal then type the following command \r\n\r\n```\r\nmongod \r\n```\r\n\r\nIt should give the following output \r\n\r\n```\r\n2016-12-03T03:00:38.986+0600 I CONTROL  [initandlisten] MongoDB starting : pid=7204 port=27017 dbpath=C:\\data\\db\\ 64-bit host=DESKTOP-4PR51E6\r\n2016-12-03T03:00:38.986+0600 I CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2\r\n2016-12-03T03:00:38.987+0600 I CONTROL  [initandlisten] db version v3.2.7\r\n.............\r\n.............\r\n2016-12-03T03:00:39.543+0600 I NETWORK  [HostnameCanonicalizationWorker] Starting hostname canonicalization worker\r\n2016-12-03T03:00:39.543+0600 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/data/db/diagnostic.data'\r\n2016-12-03T03:00:39.558+0600 I NETWORK  [initandlisten] waiting for connections on port 27017\r\n```\r\n\r\nNow you're all set to use the MongoDB service! \r\n\r\n### MongoDB Troubleshooting\r\n\r\n* Couldn't find the path \r\n\r\nAdd the `MongoDB\\Server\\3.2\\bin` folder to your system path and then try again.\r\n\r\n* ` Data directory C:\\data\\db\\ not found., terminating` \r\n\r\nQuite simple, all you need to do is to create a `db` folder there and you're good to go.\r\n\r\n### Starting Elasticsearch Server \r\n\r\nGo to `elasticsearch-5.0.0\\bin` folder then run the program `elasticsearch.bat` on windows. \r\n\r\n### Starting Kibana Server\r\n\r\nGo to `kibana-5.0.0-windows-x86\\bin` folder and run the program `kibana.bat` on windows.\r\n\r\n> All of your local server and services should be working properly.\r\n> Start crawling using the scrapy crawl command and the data will be automatically inserted to `mongo database`, `elasticsearch`, and you can get the output as either `csv` or `json` format.\r\n> You must start `elasticsearch` before `kibana`\r\n\r\n### Configuring Kibana for data acquisition and Visualization\r\n\r\nKibana server will listen to `localhost:5601` by default. So open the url in your browser. \r\n\r\n* Go to `Management`\r\n\r\n![management](https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc1.png?raw=true)\r\n\r\n* Click on `Index Patterns` and then `Add New`\r\n\r\n![indexpattern](https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc2.png?raw=true)\r\n\r\n* Remove tick from `Index contains time-based events` and  write `news*` on the **Index name or pattern** text input. Then click `Create`\r\n\r\n![index](https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc3.png?raw=true)\r\n\r\n* Then go to `Discover` and select `news*` index \r\n\r\n![selection](https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc4.gif?raw=true)\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}