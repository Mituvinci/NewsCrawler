<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Newscrawler : News crawler">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Newscrawler</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/manashmndl/NewsCrawler">View on GitHub</a>

          <h1 id="project_title">Newscrawler</h1>
          <h2 id="project_tagline">News crawler</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/manashmndl/NewsCrawler/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/manashmndl/NewsCrawler/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="bangladeshi-online-newspaper-crawler" class="anchor" href="#bangladeshi-online-newspaper-crawler" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bangladeshi Online Newspaper Crawler</h1>

<h2>
<a id="done" class="anchor" href="#done" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Done</h2>

<ul>
<li><a href="http://en.prothom-alo.com"><del>en.prothom-alo.com</del></a></li>
<li><a href="http://www.thedailystar.net"><del>thedailystar.net</del></a></li>
<li><a href="http://archive.dhakatribune.com/archive"><del>dhakatribune.com</del></a></li>
</ul>

<h1>
<a id="requirements" class="anchor" href="#requirements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h1>

<ul>
<li>Python 2k</li>
<li>Check <code>requirements.txt</code> file to find out about the dependencies</li>
</ul>

<h1>
<a id="how-to" class="anchor" href="#how-to" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to</h1>

<h2>
<a id="0-before-beginning" class="anchor" href="#0-before-beginning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>0. Before Beginning</h2>

<h3>
<a id="1-download-and-install-mongodb" class="anchor" href="#1-download-and-install-mongodb" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. <a href="https://docs.mongodb.com/v3.2/installation/">Download and Install MongoDB</a>
</h3>

<h3>
<a id="2-download-stanford-ner-and-configure-it" class="anchor" href="#2-download-stanford-ner-and-configure-it" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. <a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Download Stanford NER</a> and <a href="https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a">configure it</a>
</h3>

<h3>
<a id="3-download-and-configure-elasticsearch--kibana" class="anchor" href="#3-download-and-configure-elasticsearch--kibana" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. <a href="https://www.elastic.co/guide/index.html">Download and configure Elasticsearch &amp; Kibana</a>
</h3>

<h2>
<a id="1-installing-the-dependencies" class="anchor" href="#1-installing-the-dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Installing the dependencies</h2>

<p>Clone the repository, then at the root of the directory of the repo, open a command window/terminal and run this following command. Make sure you have <code>pip</code>.</p>

<pre><code>pip install -r requirements.txt
</code></pre>

<h2>
<a id="2-configuring-api-and-stanfordner-path" class="anchor" href="#2-configuring-api-and-stanfordner-path" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Configuring API and StanfordNER Path</h2>

<h3>
<a id="indicoio-api-configuration" class="anchor" href="#indicoio-api-configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Indicoio API Configuration</h3>

<p>At <code>credentials_and_configs/keys.py</code> file, change the API key. <strong><a href="https://indico.io/">You can create an account here and get your own API Key</a></strong>.</p>

<p>Example,</p>

<pre><code>INDICOIO_API_KEY = '8ee6432e7dc137740c40c0af8d7XXXXXX' # Replace the value with your own API Key
</code></pre>

<h3>
<a id="stanfordner-path" class="anchor" href="#stanfordner-path" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>StanfordNER Path</h3>

<p>At <code>credentials_and_configs/stanford_ner_path.py</code> file, change the paths according to the downloaded <code>NER</code> and <code>CLASSIFIER</code> paths.</p>

<p>Example,</p>

<pre><code>STANFORD_NER_PATH = 'C:\StanfordParser\stanford-ner-2015-12-09\stanford-ner.jar' #Insert your path here
STANFORD_CLASSIFIER_PATH = 'C:\StanfordParser\stanford-ner-2015-12-09\classifiers\english.all.3class.distsim.crf.ser.gz' # Insert your path here
</code></pre>

<h2>
<a id="3-running-the-spiders" class="anchor" href="#3-running-the-spiders" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Running the spiders</h2>

<p>Open a command <code>window / terminal</code> at the <code>root</code> of the folder. Run the following commands to start scraping.</p>

<h3>
<a id="4-crawling-instructions" class="anchor" href="#4-crawling-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Crawling Instructions</h3>

<h2>
<a id="spider-names" class="anchor" href="#spider-names" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spider Names</h2>

<ul>
<li>The Daily Star -&gt; <code>dailystar</code>
</li>
<li>Prothom Alo -&gt; <code>prothomalo</code>
</li>
<li>Dhaka Tribune -&gt; <code>dhakatribune</code>
</li>
</ul>

<h4>
<a id="crawl-em-all" class="anchor" href="#crawl-em-all" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Crawl 'em all</h4>

<p><strong>For Daily Star</strong></p>

<pre><code>scrapy crawl dailystar
</code></pre>

<p><strong>For Prothom Alo</strong></p>

<pre><code>scrapy crawl prothomalo
</code></pre>

<p><strong>For Dhaka Tribune</strong></p>

<pre><code>scrapy crawl dhakatribune
</code></pre>

<h4>
<a id="crawling-bounded-by-date-time" class="anchor" href="#crawling-bounded-by-date-time" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Crawling bounded by date time</h4>

<p>If I want to scrape all of the news between <code>1st January 2016</code> and <code>1st February 2016</code> my command will look like this, </p>

<p><strong>Daily Star</strong></p>

<pre><code>scrapy crawl dailystar -a start_date="01-01-2016" -a  end_date="01-02-2016"
</code></pre>

<p><strong>Prothom Alo</strong></p>

<pre><code>scrapy crawl prothomalo -a start_date="01-01-2016" -a  end_date="01-02-2016"
</code></pre>

<h4>
<a id="crawling-dhaka-tribune-by-page-range" class="anchor" href="#crawling-dhaka-tribune-by-page-range" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Crawling Dhaka Tribune by page range</h4>

<p><strong>Dhaka Tribune</strong></p>

<pre><code>scrapy crawl dhakatribune -a start_page=0 -a end_page=10
</code></pre>

<h4>
<a id="crawling-with-csvjson-output" class="anchor" href="#crawling-with-csvjson-output" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Crawling with CSV/JSON output</h4>

<p>If you want to collect all crawled data in a csv or a json file you can run this command.</p>

<p><strong>Daily Star [<code>csv</code> output]</strong></p>

<pre><code>scrapy crawl dailystar -a start_date="01-01-2016" -a end_date="01-02-2016" -o output_file_name.csv
</code></pre>

<p><strong>Daily Star [<code>json</code> output]</strong></p>

<pre><code>scrapy crawl dailystar -a start_date="01-01-2016" -a end_date="01-02-2016" -o output_file_name.json
</code></pre>

<p><strong>Dhaka Tribune [<code>csv</code> output]</strong></p>

<pre><code>scrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.csv
</code></pre>

<p><strong>Dhaka Tribune [<code>json</code> output]</strong></p>

<pre><code>scrapy crawl dhakatribune -a start_page=0 -a end_page=10 -o output_file_name.json
</code></pre>

<p><strong>Prothom Alo [<code>csv</code> output]</strong></p>

<pre><code>scrapy crawl prothomalo -a start_date="01-01-2016" -a end_date="01-02-2016" -o output_file_name.csv
</code></pre>

<p><strong>Prothom Alo [<code>json</code> output]</strong></p>

<pre><code>scrapy crawl prothomalo -a start_date="01-01-2016" -a end_date="01-02-2016" -o output_file_name.json
</code></pre>

<h2>
<a id="5-data-insertion-into-elasticsearch-and-kibana-visualization-instructions" class="anchor" href="#5-data-insertion-into-elasticsearch-and-kibana-visualization-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Data insertion into Elasticsearch and Kibana Visualization Instructions</h2>

<ul>
<li>Download and extract Kibana and Elasticsearch</li>
</ul>

<h3>
<a id="starting-mongodb-service" class="anchor" href="#starting-mongodb-service" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Starting MongoDB Service</h3>

<ul>
<li>Open CMD/Terminal then type the following command </li>
</ul>

<pre><code>mongod 
</code></pre>

<p>It should give the following output </p>

<pre><code>2016-12-03T03:00:38.986+0600 I CONTROL  [initandlisten] MongoDB starting : pid=7204 port=27017 dbpath=C:\data\db\ 64-bit host=DESKTOP-4PR51E6
2016-12-03T03:00:38.986+0600 I CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2016-12-03T03:00:38.987+0600 I CONTROL  [initandlisten] db version v3.2.7
.............
.............
2016-12-03T03:00:39.543+0600 I NETWORK  [HostnameCanonicalizationWorker] Starting hostname canonicalization worker
2016-12-03T03:00:39.543+0600 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/data/db/diagnostic.data'
2016-12-03T03:00:39.558+0600 I NETWORK  [initandlisten] waiting for connections on port 27017
</code></pre>

<p>Now you're all set to use the MongoDB service! </p>

<h3>
<a id="mongodb-troubleshooting" class="anchor" href="#mongodb-troubleshooting" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MongoDB Troubleshooting</h3>

<ul>
<li>Couldn't find the path </li>
</ul>

<p>Add the <code>MongoDB\Server\3.2\bin</code> folder to your system path and then try again.</p>

<ul>
<li>
<code>Data directory C:\data\db\ not found., terminating</code> </li>
</ul>

<p>Quite simple, all you need to do is to create a <code>db</code> folder there and you're good to go.</p>

<h3>
<a id="starting-elasticsearch-server" class="anchor" href="#starting-elasticsearch-server" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Starting Elasticsearch Server</h3>

<p>Go to <code>elasticsearch-5.0.0\bin</code> folder then run the program <code>elasticsearch.bat</code> on windows. </p>

<h3>
<a id="starting-kibana-server" class="anchor" href="#starting-kibana-server" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Starting Kibana Server</h3>

<p>Go to <code>kibana-5.0.0-windows-x86\bin</code> folder and run the program <code>kibana.bat</code> on windows.</p>

<blockquote>
<p>All of your local server and services should be working properly.
Start crawling using the scrapy crawl command and the data will be automatically inserted to <code>mongo database</code>, <code>elasticsearch</code>, and you can get the output as either <code>csv</code> or <code>json</code> format.
You must start <code>elasticsearch</code> before <code>kibana</code></p>
</blockquote>

<h3>
<a id="configuring-kibana-for-data-acquisition-and-visualization" class="anchor" href="#configuring-kibana-for-data-acquisition-and-visualization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuring Kibana for data acquisition and Visualization</h3>

<p>Kibana server will listen to <code>localhost:5601</code> by default. So open the url in your browser. </p>

<ul>
<li>Go to <code>Management</code>
</li>
</ul>

<p><img src="https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc1.png?raw=true" alt="management"></p>

<ul>
<li>Click on <code>Index Patterns</code> and then <code>Add New</code>
</li>
</ul>

<p><img src="https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc2.png?raw=true" alt="indexpattern"></p>

<ul>
<li>Remove tick from <code>Index contains time-based events</code> and  write <code>news*</code> on the <strong>Index name or pattern</strong> text input. Then click <code>Create</code>
</li>
</ul>

<p><img src="https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc3.png?raw=true" alt="index"></p>

<ul>
<li>Then go to <code>Discover</code> and select <code>news*</code> index </li>
</ul>

<p><img src="https://github.com/manashmndl/NewsCrawler/blob/master/screenshots/doc4.gif?raw=true" alt="selection"></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Newscrawler maintained by <a href="https://github.com/manashmndl">manashmndl</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
