{
  "name": "Newscrawler",
  "tagline": "News crawler",
  "body": "# Bangladeshi Online Newspaper Crawler\r\n\r\n## Currently working on:\r\n\r\n* [~~en.prothom-alo.com~~](http://en.prothom-alo.com)\r\n* [~~thedailystar.net~~](http://www.thedailystar.net)\r\n* [dhakatribune.com](http://archive.dhakatribune.com/archive)\r\n\r\n# Requirements\r\n\r\n* Python 2k\r\n* Check `requirements.txt` file to find out about the dependencies\r\n\r\n# How to \r\n\r\n## 1. Installing the dependencies\r\n\r\nClone the repository, then at the root of the directory of the repo, open a command window/terminal and run this following command. Make sure you have `pip`.\r\n\r\n```\r\npip install -r requirements.txt\r\n```\r\n\r\n## 2. Configuring API and StanfordNER Path\r\n\r\n##[TODO]\r\n\r\n\r\n## 3. Running the spiders\r\n\r\nOpen a command window /terminal at the root of the folder. Run the following commands to start scraping.\r\n\r\n### 4. Crawling Instructions\r\n\r\n## Spider Names\r\n\r\n* The Daily Star -> `dailystar`\r\n* Prothom Alo -> `prothomalo`\r\n\r\n#### Crawl 'em all\r\n\r\n**For Daily Star**\r\n```\r\nscrapy crawl dailystar\r\n```\r\n\r\n**For Prothom Alo**\r\n```\r\nscrapy crawl prothomalo\r\n```\r\n\r\n#### Crawling bounded by date time \r\n\r\nIf I want to scrape all of the news between `1st January 2016` and `1st February 2016` my command will look like this, \r\n\r\n**Daily Star**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n**Prothom Alo**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a  end_date=\"01-02-2016\"\r\n```\r\n\r\n#### Crawling with CSV/JSON output \r\n\r\nIf you want to collect all crawled data in a csv or a json file you can run this command.\r\n\r\n**Daily Star [`csv` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Daily Star [`json` output]**\r\n```\r\nscrapy crawl dailystar -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n**Prothom Alo [`csv` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.csv\r\n```\r\n\r\n**Daily Star [`csv` output]**\r\n```\r\nscrapy crawl prothomalo -a start_date=\"01-01-2016\" -a end_date=\"01-02-2016\" -o output_file_name.json\r\n```\r\n\r\n## 5. Data insertion into Elasticsearch and Kibana Visualization Instructions\r\n\r\n### [TODO]",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}